{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usbiGHIOvm-n"
      },
      "source": [
        "# Fine-Tuning vs Baseline (Gemma 2 2B Instruct)\n",
        "\n",
        "This notebook compares a baseline instruction-tuned LLM against a fine-tuned version on time-sensitive Python release questions.\n",
        "\n",
        "Sections:\n",
        "- Baseline: load the same model used in your RAG notebook and ask the 6 demo questions\n",
        "- Fine-tuning: train a LoRA/QLoRA adapter using your JSONL dataset and re-evaluate the same questions\n",
        "\n",
        "Notes:\n",
        "- Target model: `google/gemma-2-2b-it` (same as your RAG notebook)\n",
        "- Dataset: `data/processed/fine-tuning-training-data.v4.cleaned.jsonl` (or upload via Colab)\n",
        "- Designed for Google Colab (T4/L4/A100); runs with 4-bit quantization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment setup – pinned for reproducibility\n",
        "\n",
        "%pip install -q \\\n",
        "  \"numpy==1.26.4\" \\\n",
        "  \"protobuf==4.25.3\" \\\n",
        "  \"transformers==4.43.3\" \\\n",
        "  \"accelerate==0.29.3\" \\\n",
        "  \"peft==0.11.1\" \\\n",
        "  \"trl==0.9.6\" \\\n",
        "  bitsandbytes datasets sentencepiece pandas\n",
        "\n",
        "import numpy as np\n",
        "import torch, transformers, datasets, peft, trl, google.protobuf\n",
        "\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"datasets:\", datasets.__version__)\n",
        "print(\"peft:\", peft.__version__)\n",
        "print(\"trl:\", trl.__version__)\n",
        "print(\"protobuf:\", google.protobuf.__version__)\n"
      ],
      "metadata": {
        "id": "2S3IdWBb3IF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b98d9c-0853-4ee4-df00-61f4a595b26b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/294.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m286.7/294.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mNumPy: 1.26.4\n",
            "PyTorch: 2.9.0+cu126\n",
            "transformers: 4.43.3\n",
            "datasets: 4.4.1\n",
            "peft: 0.11.1\n",
            "trl: 0.9.6\n",
            "protobuf: 4.25.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eL3vhLw1vm-q"
      },
      "outputs": [],
      "source": [
        "# Optional: Hugging Face login (only if your model is gated)\n",
        "from huggingface_hub import login\n",
        "login(token=\""Access Token"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "b174e90066ff435d9c152f71e2988cf1",
            "cad12db613924bf780d66085aabd7302",
            "a14ef22beefd4332a3f4c2f2d9c2c342",
            "e2e512ddca744b1f966d1a70d53ce3a4",
            "03a9346dadc345a2a4e59079a888a392",
            "1cd67f5908974afabd6cdcedc202ecd8",
            "cc9dee55c1874f13808c0740723204cb",
            "69f822cbfa9246cc99537bab831159d7",
            "b3f26c64f34043a7b1f93bfa51d71912",
            "e0d3549dd3904d38bad8db27956fae4a",
            "569564699a8544d69afc6978cf48b6c1"
          ]
        },
        "id": "REIbey4Svm-q",
        "outputId": "458a3b36-bdae-4f4c-cfe3-0c25f84a149e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Model: google/gemma-2-2b-it\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b174e90066ff435d9c152f71e2988cf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline model loaded.\n"
          ]
        }
      ],
      "source": [
        "import os, sys, json, math\n",
        "from typing import List, Dict, Optional\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "\n",
        "try:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    _bnb_available = True\n",
        "except Exception:\n",
        "    _bnb_available = False\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_ID = \"google/gemma-2-2b-it\"  # same baseline as RAG notebook\n",
        "\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"Model:\", MODEL_ID)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "\n",
        "if DEVICE == \"cuda\" and _bnb_available:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
        "    )\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "else:\n",
        "    dtype = torch.float32 if DEVICE == \"cpu\" else torch.float16\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=dtype)\n",
        "    base_model.to(DEVICE)\n",
        "\n",
        "base_model.eval()\n",
        "print(\"Baseline model loaded.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PJLaz-gbvm-r"
      },
      "outputs": [],
      "source": [
        "GEN_CFG = {\n",
        "    \"max_new_tokens\": 600,\n",
        "    \"temperature\": 0.3,\n",
        "    \"top_p\": 0.9,\n",
        "    \"repetition_penalty\": 1.1,\n",
        "}\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a Python programming assistant.\"\n",
        "\n",
        "\n",
        "def _format_chat(messages: List[Dict[str, str]], add_generation_prompt: bool = True) -> Dict[str, torch.Tensor]:\n",
        "    if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
        "        effective_messages = messages\n",
        "        if messages and messages[0].get(\"role\") == \"system\":\n",
        "            system_text = messages[0][\"content\"]\n",
        "            effective_messages = messages[1:]\n",
        "            if effective_messages and effective_messages[0].get(\"role\") == \"user\":\n",
        "                effective_messages = effective_messages.copy()\n",
        "                effective_messages[0] = {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"{system_text}\\n\\n{effective_messages[0]['content']}\"\n",
        "                }\n",
        "            else:\n",
        "                effective_messages = [{\"role\": \"user\", \"content\": system_text}]\n",
        "        prompt_text = tokenizer.apply_chat_template(\n",
        "            effective_messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=add_generation_prompt\n",
        "        )\n",
        "    else:\n",
        "        sys_msg = \"\"\n",
        "        if messages and messages[0].get(\"role\") == \"system\":\n",
        "            sys_msg = f\"System: {messages[0]['content']}\\n\"\n",
        "            user_msgs = messages[1:]\n",
        "        else:\n",
        "            user_msgs = messages\n",
        "        convo = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in user_msgs])\n",
        "        prompt_text = (sys_msg + convo + (\"\\nAssistant:\" if add_generation_prompt else \"\"))\n",
        "\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_from_messages(\n",
        "    model,\n",
        "    messages: List[Dict[str, str]],\n",
        "    max_new_tokens: int = GEN_CFG[\"max_new_tokens\"],\n",
        "    temperature: float = GEN_CFG[\"temperature\"],\n",
        "    top_p: float = GEN_CFG[\"top_p\"],\n",
        "    repetition_penalty: float = GEN_CFG[\"repetition_penalty\"],\n",
        ") -> str:\n",
        "    inputs = _format_chat(messages, add_generation_prompt=True)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    gen_ids = outputs[0][input_len:]\n",
        "    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def ask_baseline(question: str, system_prompt: Optional[str] = SYSTEM_PROMPT, **gen_kwargs) -> str:\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    return generate_from_messages(base_model, messages, **gen_kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLmSJuXYvm-r",
        "outputId": "49c7a4f5-632e-4487-cace-b4c9884c3345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q1: What was added in Python 3.12.2 released in March 2024?\n",
            "\n",
            "I do not have access to real-time information, including specific release notes for software updates like Python 3.12.2.  \n",
            "\n",
            "**To find the details about what was added in Python 3.12.2, I recommend checking these resources:**\n",
            "\n",
            "* **The official Python website:** https://www.python.org/\n",
            "* **Python's Release Notes page:** https://docs.python.org/3/whatsnew/\n",
            "* **The Python Enhancement Proposals (PEP) repository:** https://peps.python.org/\n",
            "\n",
            "\n",
            "These sources will provide you with the most accurate and up-to-date information on the changes made in Python 3.12.2.\n"
          ]
        }
      ],
      "source": [
        "DEMO_QUESTIONS = [\n",
        "    \"What was added in Python 3.12.2 released in March 2024?\",\n",
        "]\n",
        "\n",
        "for i, q in enumerate(DEMO_QUESTIONS, 1):\n",
        "    print(f\"\\nQ{i}: {q}\\n\")\n",
        "    ans = ask_baseline(q)\n",
        "    print(ans)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvZM2pygvm-s"
      },
      "source": [
        "---\n",
        "\n",
        "## Fine-tuning with your JSONL dataset (QLoRA)\n",
        "We will fine-tune the same baseline model using your dataset:\n",
        "- Preferred path: `data/processed/fine-tuning-training-data.v4.cleaned.jsonl`\n",
        "- If not found, you can upload the file when running in Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb1HmqTnvm-s",
        "outputId": "79413b33-277a-4765-d756-56acf44fbaea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dataset: /content/fine-tuning-training-data.v4.cleaned.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Locate dataset (adjust this if running locally vs Colab)\n",
        "from pathlib import Path\n",
        "\n",
        "# Preferred local path\n",
        "DATA_PATHS = [\n",
        "    Path(\"data/processed/fine-tuning-training-data.v4.cleaned.jsonl\"),\n",
        "    Path(\"/content/data/processed/fine-tuning-training-data.v4.cleaned.jsonl\"),\n",
        "    Path(\"/content/fine-tuning-training-data.v4.cleaned.jsonl\"),\n",
        "]\n",
        "\n",
        "DATA_PATH = None\n",
        "for p in DATA_PATHS:\n",
        "    if p.exists():\n",
        "        DATA_PATH = str(p)\n",
        "        break\n",
        "\n",
        "if DATA_PATH is None:\n",
        "    print(\"Dataset not found at default paths. Upload the JSONL file or mount drive and set DATA_PATH manually.\")\n",
        "else:\n",
        "    print(\"Using dataset:\", DATA_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37S_sMfJvm-s",
        "outputId": "c673331b-1414-4144-d9ee-c76a41a3be6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['messages', 'source_sheet'],\n",
            "    num_rows: 1289\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load JSONL chat-style dataset with datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = None\n",
        "if DATA_PATH is not None:\n",
        "    dataset = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
        "    print(dataset)\n",
        "else:\n",
        "    raise FileNotFoundError(\"Please set DATA_PATH to your JSONL file.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atqZdd5Xvm-s",
        "outputId": "8de60b6a-e3ba-4398-aee5-5ebbf3462735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 1289\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Formatting function: convert messages -> chat template string\n",
        "# We keep it simple for demo: train on full conversation (prompt + answer)\n",
        "# For production, you can mask inputs using TRL's response_template.\n",
        "\n",
        "def format_example(example):\n",
        "    msgs = example.get(\"messages\")\n",
        "    if not msgs:\n",
        "        return \"\"\n",
        "    try:\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            msgs,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "    except Exception:\n",
        "        # Fallback: naive concatenation\n",
        "        parts = []\n",
        "        for m in msgs:\n",
        "            role = m.get(\"role\", \"user\")\n",
        "            parts.append(f\"{role}: {m.get('content','')}\")\n",
        "        text = \"\\n\".join(parts)\n",
        "    # Ensure an EOS to bound samples\n",
        "    eos = tokenizer.eos_token or \"</s>\"\n",
        "    return text + eos\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "def formatting_func(examples):\n",
        "    texts = []\n",
        "    for msgs in examples[\"messages\"]:\n",
        "        texts.append(format_example({\"messages\": msgs}))\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Map to a text field for SFTTrainer\n",
        "processed = dataset.map(formatting_func, batched=True, remove_columns=dataset.column_names)\n",
        "print(processed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "66d0e5f07e38467095cc9a139bfe104d",
            "f9b03f9ee387417c9587d59a67e680a0",
            "07b5aa5e3197479fa8204b94afb55b51",
            "096197c1e7e341fa87908dc7d5b28bc8",
            "4ba0eb7587f0438f9ccdc063e13170a0",
            "1d69871cff214ecfb6aa2fd7493b9566",
            "6bf595d67aad4256b55537820e9934fb",
            "6762ffb8b5834b0187e0c9b6f59ad936",
            "9faa445feb464b62902d30197d516735",
            "820f7507c1924bc48996428559ed187e",
            "5e4f3639d3e440799253181f0bf94cc6"
          ]
        },
        "id": "fiVvwWZzvm-t",
        "outputId": "bfc01e51-0222-414b-d75e-c52f7a437eec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66d0e5f07e38467095cc9a139bfe104d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "MODEL_NAME = \"google/gemma-2-2b-it\"   # same as before\n",
        "OUTPUT_DIR = \"outputs/gemma2-2b-it-lora\"\n",
        "\n",
        "# 1. 4-bit quantization config (QLoRA style)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        ")\n",
        "\n",
        "# 2. Reload base model cleanly in 4-bit\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Disable cache for training\n",
        "base_model.config.use_cache = False\n",
        "\n",
        "# Prepare model for k-bit training (sets up gradients correctly)\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "\n",
        "# 3. LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# 4. Wrap with LoRA – this creates trainable adapter params\n",
        "base_model = get_peft_model(base_model, lora_config)\n",
        "base_model.print_trainable_parameters()\n",
        "\n",
        "# 5. Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 6. Tokenize your dataset (processed must have \"text\" column)\n",
        "#    ↓↓↓ REDUCED max_length to 1024 to save memory\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=1024,           # was 2048\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "tokenized_dataset = processed.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=processed.column_names,\n",
        ")\n",
        "\n",
        "# 7. Data collator for causal LM\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# 8. TrainingArguments – memory friendly settings\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,    # was 2\n",
        "    gradient_accumulation_steps=16,   # keep effective batch size similar\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    bf16=torch.cuda.is_available(),\n",
        "    fp16=not torch.cuda.is_available(),\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    gradient_checkpointing=True,      # ON now to save memory\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# 9. Clear any leftover cache before starting\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 10. Trainer\n",
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# 11. Train\n",
        "trainer.train()\n",
        "\n",
        "# 12. Save adapter + tokenizer\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved adapter to:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip1kp1XNvm-t"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned adapter for inference\n",
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
        "ft_model.eval()\n",
        "print(\"Loaded LoRA adapter for inference.\")\n",
        "\n",
        "\n",
        "def ask_finetuned(question: str, system_prompt: Optional[str] = SYSTEM_PROMPT, **gen_kwargs) -> str:\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    return generate_from_messages(ft_model, messages, **gen_kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UClXXE5Evm-t"
      },
      "outputs": [],
      "source": [
        "# Re-ask the same demo questions: baseline vs fine-tuned\n",
        "for i, q in enumerate(DEMO_QUESTIONS, 1):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Q{i}: {q}\")\n",
        "    print(\"-\"*80)\n",
        "    base = ask_baseline(q)\n",
        "    print(\"Baseline:\\n\", base)\n",
        "    print(\"-\"*80)\n",
        "    ft = ask_finetuned(q)\n",
        "    print(\"Fine-tuned:\\n\", ft)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UX2-rzqvm-u"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b174e90066ff435d9c152f71e2988cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cad12db613924bf780d66085aabd7302",
              "IPY_MODEL_a14ef22beefd4332a3f4c2f2d9c2c342",
              "IPY_MODEL_e2e512ddca744b1f966d1a70d53ce3a4"
            ],
            "layout": "IPY_MODEL_03a9346dadc345a2a4e59079a888a392"
          }
        },
        "cad12db613924bf780d66085aabd7302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cd67f5908974afabd6cdcedc202ecd8",
            "placeholder": "​",
            "style": "IPY_MODEL_cc9dee55c1874f13808c0740723204cb",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a14ef22beefd4332a3f4c2f2d9c2c342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69f822cbfa9246cc99537bab831159d7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3f26c64f34043a7b1f93bfa51d71912",
            "value": 2
          }
        },
        "e2e512ddca744b1f966d1a70d53ce3a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0d3549dd3904d38bad8db27956fae4a",
            "placeholder": "​",
            "style": "IPY_MODEL_569564699a8544d69afc6978cf48b6c1",
            "value": " 2/2 [00:05&lt;00:00,  2.23s/it]"
          }
        },
        "03a9346dadc345a2a4e59079a888a392": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd67f5908974afabd6cdcedc202ecd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9dee55c1874f13808c0740723204cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69f822cbfa9246cc99537bab831159d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3f26c64f34043a7b1f93bfa51d71912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0d3549dd3904d38bad8db27956fae4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "569564699a8544d69afc6978cf48b6c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66d0e5f07e38467095cc9a139bfe104d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9b03f9ee387417c9587d59a67e680a0",
              "IPY_MODEL_07b5aa5e3197479fa8204b94afb55b51",
              "IPY_MODEL_096197c1e7e341fa87908dc7d5b28bc8"
            ],
            "layout": "IPY_MODEL_4ba0eb7587f0438f9ccdc063e13170a0"
          }
        },
        "f9b03f9ee387417c9587d59a67e680a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d69871cff214ecfb6aa2fd7493b9566",
            "placeholder": "​",
            "style": "IPY_MODEL_6bf595d67aad4256b55537820e9934fb",
            "value": "Loading checkpoint shards:  50%"
          }
        },
        "07b5aa5e3197479fa8204b94afb55b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6762ffb8b5834b0187e0c9b6f59ad936",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9faa445feb464b62902d30197d516735",
            "value": 1
          }
        },
        "096197c1e7e341fa87908dc7d5b28bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_820f7507c1924bc48996428559ed187e",
            "placeholder": "​",
            "style": "IPY_MODEL_5e4f3639d3e440799253181f0bf94cc6",
            "value": " 1/2 [00:15&lt;00:15, 15.09s/it]"
          }
        },
        "4ba0eb7587f0438f9ccdc063e13170a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d69871cff214ecfb6aa2fd7493b9566": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bf595d67aad4256b55537820e9934fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6762ffb8b5834b0187e0c9b6f59ad936": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9faa445feb464b62902d30197d516735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "820f7507c1924bc48996428559ed187e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e4f3639d3e440799253181f0bf94cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}