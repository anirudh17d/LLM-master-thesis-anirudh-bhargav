{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNLKtCNziXs2"
      },
      "source": [
        "# Baseline with RAG Integration\n",
        "\n",
        "This notebook extends the baseline LLM with retrieval-augmented generation (RAG).\n",
        "\n",
        "Workflow:\n",
        "- Load the same baseline model used in the prompt-engineering notebook\n",
        "- Ingest a user-provided knowledge base (e.g., JSONL/CSV/TXT with recent release info)\n",
        "- Build embeddings + FAISS index\n",
        "- Compare answers: baseline `ask()` vs RAG `ask_rag()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b1kMhD2iXs4",
        "outputId": "2f751ba0-4fa8-4573-86c7-0d190beb74ca",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Installs (Colab/Local)\n",
        "%pip -q install -U transformers accelerate sentencepiece bitsandbytes sentence-transformers faiss-cpu pandas python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UtHYE9R5iXs5"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ytEsBj1YiXs6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkKEOi88iXs6"
      },
      "outputs": [],
      "source": [
        "# Optional: Hugging Face login (needed if your chosen model is gated)\n",
        "from huggingface_hub import login\n",
        "login(token=\"access token here\")  # uncomment and add your token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "94f40a496c4046ab8280e5bb56245250",
            "6f46eed1c3f448028e71f4d8c22cd8b0",
            "ead940ae10b6490aa4fc6832c1694eda",
            "8d78114faa4e4a008fa54bbf8b360c59",
            "32379767a7d442809e65589fcd073e9c",
            "196ec6ec2e9e471f92b2e853304d0518",
            "0f979ff08f834ea5a7466e7b542cc96a",
            "15cc969372474ab18ec714e37ebda024",
            "4eb3b128fa9440f991000c7aed65fe35",
            "d782b03ee287488695c595f32f8f1171",
            "5067d2f6e1ae42d989ffba6fc4eba902"
          ]
        },
        "id": "Yz_ulYEniXs6",
        "outputId": "b8f50d7b-cce3-4510-a3bc-57b6569359a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Model: google/gemma-2-2b-it\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94f40a496c4046ab8280e5bb56245250",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "try:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    _bnb_available = True\n",
        "except Exception:\n",
        "    _bnb_available = False\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_ID = \"google/gemma-2-2b-it\"  # same baseline\n",
        "\n",
        "# Generation defaults\n",
        "GEN_CFG = {\n",
        "    \"max_new_tokens\": 800,\n",
        "    \"temperature\": 0.3,\n",
        "    \"top_p\": 0.9,\n",
        "    \"repetition_penalty\": 1.1,\n",
        "}\n",
        "\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"Model:\", MODEL_ID)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "\n",
        "if DEVICE == \"cuda\" and _bnb_available:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "else:\n",
        "    dtype = torch.float32 if DEVICE == \"cpu\" else torch.float16\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=dtype)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "model.eval()\n",
        "print(\"Model loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7xYImjlWiXs7"
      },
      "outputs": [],
      "source": [
        "PYTHON_ASSISTANT_SYSTEM_PROMPT = \"You are a Python programming assistant.\"\n",
        "\n",
        "# Chat formatting with system-role fallback for templates that don't support 'system'\n",
        "def _format_chat(messages: List[Dict[str, str]], add_generation_prompt: bool = True) -> Dict[str, torch.Tensor]:\n",
        "    if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
        "        effective_messages = messages\n",
        "        if messages and messages[0].get(\"role\") == \"system\":\n",
        "            system_text = messages[0][\"content\"]\n",
        "            effective_messages = messages[1:]\n",
        "            if effective_messages and effective_messages[0].get(\"role\") == \"user\":\n",
        "                effective_messages = effective_messages.copy()\n",
        "                effective_messages[0] = {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"{system_text}\\n\\n{effective_messages[0]['content']}\"\n",
        "                }\n",
        "            else:\n",
        "                effective_messages = [{\"role\": \"user\", \"content\": system_text}]\n",
        "        prompt_text = tokenizer.apply_chat_template(\n",
        "            effective_messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=add_generation_prompt\n",
        "        )\n",
        "    else:\n",
        "        sys_msg = \"\"\n",
        "        if messages and messages[0].get(\"role\") == \"system\":\n",
        "            sys_msg = f\"System: {messages[0]['content']}\\n\"\n",
        "            user_msgs = messages[1:]\n",
        "        else:\n",
        "            user_msgs = messages\n",
        "        convo = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in user_msgs])\n",
        "        prompt_text = (sys_msg + convo + (\"\\nAssistant:\" if add_generation_prompt else \"\"))\n",
        "\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_from_messages(\n",
        "    messages: List[Dict[str, str]],\n",
        "    max_new_tokens: int = GEN_CFG[\"max_new_tokens\"],\n",
        "    temperature: float = GEN_CFG[\"temperature\"],\n",
        "    top_p: float = GEN_CFG[\"top_p\"],\n",
        "    repetition_penalty: float = GEN_CFG[\"repetition_penalty\"],\n",
        ") -> str:\n",
        "    inputs = _format_chat(messages, add_generation_prompt=True)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    gen_ids = outputs[0][input_len:]\n",
        "    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "    return text.strip()\n",
        "\n",
        "def ask(question: str, system_prompt: Optional[str] = PYTHON_ASSISTANT_SYSTEM_PROMPT, **gen_kwargs) -> str:\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    return generate_from_messages(messages, **gen_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "J2p7D6oJiXs7"
      },
      "outputs": [],
      "source": [
        "# Knowledge base ingestion\n",
        "# Accepts: JSONL (one object per line with fields: id, content, [title|version|urls...])\n",
        "#          CSV (column 'content' required)\n",
        "#          TXT/MD (each file is one document)\n",
        "from pathlib import Path\n",
        "from typing import Union\n",
        "\n",
        "def read_jsonl(path: Union[str, Path]) -> List[Dict[str, str]]:\n",
        "    records = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            obj = json.loads(line)\n",
        "            content = obj.get(\"content\") or obj.get(\"text\") or obj.get(\"body\")\n",
        "            if not content:\n",
        "                continue\n",
        "            records.append({\n",
        "                \"id\": obj.get(\"id\") or obj.get(\"_id\") or str(len(records)),\n",
        "                \"title\": obj.get(\"title\") or \"\",\n",
        "                \"content\": content,\n",
        "                \"meta\": {k: v for k, v in obj.items() if k not in {\"id\", \"_id\", \"title\", \"content\", \"text\", \"body\"}}\n",
        "            })\n",
        "    return records\n",
        "\n",
        "def read_csv(path: Union[str, Path]) -> List[Dict[str, str]]:\n",
        "    df = pd.read_csv(path)\n",
        "    if \"content\" not in df.columns:\n",
        "        raise ValueError(\"CSV must have a 'content' column\")\n",
        "    recs = []\n",
        "    for i, row in df.iterrows():\n",
        "        recs.append({\n",
        "            \"id\": str(row.get(\"id\", i)),\n",
        "            \"title\": str(row.get(\"title\", \"\")),\n",
        "            \"content\": str(row[\"content\"]),\n",
        "            \"meta\": {k: row[k] for k in df.columns if k not in {\"id\", \"title\", \"content\"}}\n",
        "        })\n",
        "    return recs\n",
        "\n",
        "def read_texts(paths: List[Union[str, Path]]) -> List[Dict[str, str]]:\n",
        "    recs = []\n",
        "    for p in paths:\n",
        "        p = Path(p)\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "        recs.append({\n",
        "            \"id\": p.stem,\n",
        "            \"title\": p.name,\n",
        "            \"content\": text,\n",
        "            \"meta\": {\"path\": str(p)}\n",
        "        })\n",
        "    return recs\n",
        "\n",
        "def load_kb(path_or_dir: Union[str, Path]) -> List[Dict[str, str]]:\n",
        "    p = Path(path_or_dir)\n",
        "    if p.is_dir():\n",
        "        files = list(p.rglob(\"*.jsonl\")) + list(p.rglob(\"*.csv\")) + list(p.rglob(\"*.txt\")) + list(p.rglob(\"*.md\"))\n",
        "        recs: List[Dict[str, str]] = []\n",
        "        for fpath in files:\n",
        "            if fpath.suffix == \".jsonl\":\n",
        "                recs.extend(read_jsonl(fpath))\n",
        "            elif fpath.suffix == \".csv\":\n",
        "                recs.extend(read_csv(fpath))\n",
        "            else:\n",
        "                recs.extend(read_texts([fpath]))\n",
        "        return recs\n",
        "    else:\n",
        "        if p.suffix == \".jsonl\":\n",
        "            return read_jsonl(p)\n",
        "        if p.suffix == \".csv\":\n",
        "            return read_csv(p)\n",
        "        if p.suffix in {\".txt\", \".md\"}:\n",
        "            return read_texts([p])\n",
        "        raise ValueError(\"Unsupported KB format; use .jsonl, .csv, .txt, .md or a directory containing them\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRb4oZvMiXs8",
        "outputId": "331c2748-9ca3-4bf3-b5ac-0bafd0bf2cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        }
      ],
      "source": [
        "# Embeddings + FAISS index\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"  # small, fast\n",
        "\n",
        "class VectorStore:\n",
        "    def __init__(self, dim: int):\n",
        "        self.dim = dim\n",
        "        self.index = faiss.IndexFlatIP(dim)  # cosine via normalized dot product\n",
        "        self.docs: List[Dict[str, str]] = []\n",
        "\n",
        "    def add(self, embeddings: np.ndarray, docs: List[Dict[str, str]]):\n",
        "        # Normalize for cosine similarity\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "        self.docs.extend(docs)\n",
        "\n",
        "    def search(self, query_emb: np.ndarray, k: int = 5) -> List[Tuple[int, float]]:\n",
        "        faiss.normalize_L2(query_emb)\n",
        "        D, I = self.index.search(query_emb, k)\n",
        "        return list(zip(I[0].tolist(), D[0].tolist()))\n",
        "\n",
        "# Build embeds\n",
        "embedder = SentenceTransformer(EMBED_MODEL_ID)\n",
        "emb_dim = embedder.get_sentence_embedding_dimension()\n",
        "store = VectorStore(dim=emb_dim)\n",
        "\n",
        "# Helper to (re)build index from records\n",
        "def build_index(records: List[Dict[str, str]], batch_size: int = 64):\n",
        "    texts = [r[\"content\"] for r in records]\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        chunk = texts[i:i+batch_size]\n",
        "        vecs = embedder.encode(chunk, convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=False)\n",
        "        embeddings.append(vecs)\n",
        "    if not embeddings:\n",
        "        return\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    store.add(embeddings, records)\n",
        "\n",
        "print(\"Embedding model:\", EMBED_MODEL_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gTGYmA66iXs8"
      },
      "outputs": [],
      "source": [
        "# Retrieval and RAG ask()\n",
        "from textwrap import dedent\n",
        "\n",
        "def embed(texts: List[str]) -> np.ndarray:\n",
        "    vecs = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=False)\n",
        "    return vecs\n",
        "\n",
        "def retrieve(query: str, k: int = 4) -> List[Dict[str, str]]:\n",
        "    qvec = embed([query])\n",
        "    hits = store.search(qvec, k=k)\n",
        "    results = []\n",
        "    for idx, score in hits:\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        doc = store.docs[idx]\n",
        "        results.append({\"score\": float(score), **doc})\n",
        "    return results\n",
        "\n",
        "def make_context(snippets: List[Dict[str, str]]) -> str:\n",
        "    blocks = []\n",
        "    for s in snippets:\n",
        "        title = s.get(\"title\") or s.get(\"id\") or \"\"\n",
        "        prefix = f\"Title: {title}\\n\" if title else \"\"\n",
        "        blocks.append(prefix + s[\"content\"])\n",
        "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
        "\n",
        "def answer_with_context(question: str, context: str) -> str:\n",
        "    sys = PYTHON_ASSISTANT_SYSTEM_PROMPT\n",
        "    prompt = dedent(f\"\"\"\n",
        "    Use the context below to answer the user question. If the answer is not in the context, say you don't know.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\")\n",
        "    return ask(prompt, system_prompt=sys, max_new_tokens=GEN_CFG[\"max_new_tokens\"], temperature=GEN_CFG[\"temperature\"])\n",
        "\n",
        "# Public API\n",
        "\n",
        "def ask_baseline(question: str) -> str:\n",
        "    return ask(question)\n",
        "\n",
        "def ask_rag(question: str, k: int = 4) -> Tuple[str, List[Dict[str, str]]]:\n",
        "    snippets = retrieve(question, k=k)\n",
        "    context = make_context(snippets)\n",
        "    answer = answer_with_context(question, context)\n",
        "    return answer, snippets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ADePJqliXs9",
        "outputId": "361eb9e5-2728-42a2-ee8f-68e998eb63ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 7 records from KB.\n",
            "Indexed 7 documents.\n",
            "\n",
            "Q: What were the documented release blockers and notable open issues listed before the Python 3.13.1 release, and which were resolved by that release?\n",
            "\n",
            "Baseline:\n",
            " I do not have access to real-time information, including specific details about software releases like the Python 3.13.1 release notes.  My knowledge is based on the data I was trained on, which has a cutoff point. \n",
            "\n",
            "**To find the information you're looking for, I recommend checking these resources:**\n",
            "\n",
            "* **The official Python website:** https://www.python.org/\n",
            "* **The Python 3.13.1 Release Notes:** You can usually find this on the official Python website or in the release announcement.\n",
            "* **GitHub:** The Python project's GitHub repository might have detailed release notes: https://github.com/python/cpython\n",
            "\n",
            "\n",
            "These sources will provide the most accurate and up-to-date information regarding documented release blockers and open issues addressed in the Python 3.13.1 release.\n",
            "\n",
            "RAG:\n",
            " According to the provided text, these were the documented release blockers and notable open issues before the Python 3.13.1 release:\n",
            "\n",
            "* **Managed Dict Crash:**  A crash during managed dict rematerialization.\n",
            "* **PyEval_SetTrace Segfault:** A segmentation fault when `PyEval_SetTrace` was called with a null Python frame.\n",
            "* **REPL History/Cache:** Multi-line history and screen cache issues in the interactive REPL.\n",
            "* **Free-Threaded Assertion:** A negative refcount assertion failure in the free-threaded build.\n",
            "* **Type Lookup Race:** A specific race condition in `_PyType_Lookup`.\n",
            "\n",
            "These issues were all tracked under the \"release blocker\" label and the 3.13.1 milestone. \n",
            "\n",
            "The text states that these issues were **resolved** by the 3.13.1 release.\n",
            "\n",
            "Context snippets used (top-4):\n",
            "[1] id=q6_3131_release_blockers title=Python 3.13.1 Release Blockers and Notable Resolved Issues score=0.709\n",
            "[2] id=q3_py3132_release_details title=Python 3.13.2 Release Details (Date, Issues, Regressions) score=0.677\n",
            "[3] id=q1_py3122 title=Python 3.12.2 (March 2024) – Patch contents score=0.642\n",
            "[4] id=q2_py3123 title=Python 3.12.3 (April 2024) – Bug fixes and security advisories score=0.550\n"
          ]
        }
      ],
      "source": [
        "# Demo: Load your KB, build index, compare baseline vs RAG\n",
        "# 1) Point to a path or upload files via the Colab file browser\n",
        "KB_PATH = \"/content/updated_python_kb.jsonl\"  # change to your folder or a specific .jsonl/.csv/.txt\n",
        "\n",
        "# Example: you can also mount Google Drive in Colab and set KB_PATH accordingly\n",
        "\n",
        "records = load_kb(KB_PATH)\n",
        "print(f\"Loaded {len(records)} records from KB.\")\n",
        "\n",
        "# 2) Build (or rebuild) the FAISS index\n",
        "store = VectorStore(dim=emb_dim)\n",
        "build_index(records)\n",
        "print(f\"Indexed {len(store.docs)} documents.\")\n",
        "\n",
        "# 3) Ask a question without context (baseline) vs with RAG\n",
        "question = \"What were the documented release blockers and notable open issues listed before the Python 3.13.1 release, and which were resolved by that release?\"\n",
        "\n",
        "\n",
        "print(\"\\nQ:\", question)\n",
        "base_answer = ask_baseline(question)\n",
        "rag_answer, used = ask_rag(question, k=4)\n",
        "print(\"\\nBaseline:\\n\", base_answer)\n",
        "print(\"\\nRAG:\\n\", rag_answer)\n",
        "print(\"\\nContext snippets used (top-4):\")\n",
        "for i, snip in enumerate(used, 1):\n",
        "        print(f\"[{i}] id={snip.get('id')} title={snip.get('title','')[:80]} score={snip.get('score'):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4jJNJDzllyR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f979ff08f834ea5a7466e7b542cc96a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15cc969372474ab18ec714e37ebda024": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "196ec6ec2e9e471f92b2e853304d0518": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32379767a7d442809e65589fcd073e9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb3b128fa9440f991000c7aed65fe35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5067d2f6e1ae42d989ffba6fc4eba902": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f46eed1c3f448028e71f4d8c22cd8b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_196ec6ec2e9e471f92b2e853304d0518",
            "placeholder": "​",
            "style": "IPY_MODEL_0f979ff08f834ea5a7466e7b542cc96a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8d78114faa4e4a008fa54bbf8b360c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d782b03ee287488695c595f32f8f1171",
            "placeholder": "​",
            "style": "IPY_MODEL_5067d2f6e1ae42d989ffba6fc4eba902",
            "value": " 2/2 [00:22&lt;00:00,  9.44s/it]"
          }
        },
        "94f40a496c4046ab8280e5bb56245250": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f46eed1c3f448028e71f4d8c22cd8b0",
              "IPY_MODEL_ead940ae10b6490aa4fc6832c1694eda",
              "IPY_MODEL_8d78114faa4e4a008fa54bbf8b360c59"
            ],
            "layout": "IPY_MODEL_32379767a7d442809e65589fcd073e9c"
          }
        },
        "d782b03ee287488695c595f32f8f1171": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ead940ae10b6490aa4fc6832c1694eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15cc969372474ab18ec714e37ebda024",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4eb3b128fa9440f991000c7aed65fe35",
            "value": 2
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
