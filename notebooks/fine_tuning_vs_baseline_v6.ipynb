{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning vs Baseline (Gemma 2 2B Instruct) — v6\n",
        "\n",
        "This version mirrors v5 but replaces the demo question arrays with two manual query cells:\n",
        "\n",
        "- One after the baseline load to ask any single question to the baseline model.\n",
        "- One after the fine‑tuned adapter load to ask any single question to the fine‑tuned model.\n",
        "\n",
        "All dataset, training, and environment settings are unchanged.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Colab bootstrap: stable + idempotent (runs fast if already satisfied) ===\n",
        "import os, sys, subprocess\n",
        "\n",
        "# 1) Define what you need (avoid pinning numpy/pandas on Colab)\n",
        "REQ = {\n",
        "    \"transformers\": \"4.43.3\",\n",
        "    \"accelerate\": \"0.29.3\",\n",
        "    \"peft\": \"0.11.1\",\n",
        "    \"trl\": \"0.9.6\",\n",
        "    \"datasets\": None,\n",
        "    \"sentencepiece\": None,\n",
        "    \"bitsandbytes\": None,\n",
        "}\n",
        "\n",
        "def _get_ver(pkg_name: str):\n",
        "    try:\n",
        "        import importlib.metadata as md\n",
        "        return md.version(pkg_name)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _needs_install():\n",
        "    for pkg, ver in REQ.items():\n",
        "        v = _get_ver(pkg)\n",
        "        if v is None:\n",
        "            return True\n",
        "        if ver is not None and v != ver:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def _pip_install():\n",
        "    pkgs = []\n",
        "    for pkg, ver in REQ.items():\n",
        "        pkgs.append(f\"{pkg}=={ver}\" if ver else pkg)\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", \"--no-cache-dir\"] + pkgs\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# 2) Install only if needed, then restart ONCE (prevents numpy ABI mixups)\n",
        "if _needs_install():\n",
        "    print(\"Installing / fixing environment...\")\n",
        "    _pip_install()\n",
        "\n",
        "    # Restart once to ensure compiled deps (numpy/bitsandbytes/etc.) load cleanly\n",
        "    if not os.environ.get(\"COLAB_BOOTSTRAP_RESTARTED\"):\n",
        "        os.environ[\"COLAB_BOOTSTRAP_RESTARTED\"] = \"1\"\n",
        "        print(\"Restarting runtime once for a clean import state...\")\n",
        "        os._exit(0)\n",
        "\n",
        "print(\"Environment OK.\")\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"transformers:\", _get_ver(\"transformers\"))\n",
        "print(\"accelerate:\", _get_ver(\"accelerate\"))\n",
        "print(\"peft:\", _get_ver(\"peft\"))\n",
        "print(\"trl:\", _get_ver(\"trl\"))\n",
        "print(\"datasets:\", _get_ver(\"datasets\"))\n",
        "print(\"bitsandbytes:\", _get_ver(\"bitsandbytes\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\""Access Token"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load baseline model (quantized if possible)\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "try:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    _bnb_available = True\n",
        "except Exception:\n",
        "    _bnb_available = False\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_ID = \"google/gemma-2-2b-it\"\n",
        "\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"Model:\", MODEL_ID)\n",
        "\n",
        "# Tokenizer\n",
        "baseline_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "\n",
        "# Prefer fp16 compute on CUDA (T4 safe), no bf16 assumption\n",
        "if DEVICE == \"cuda\" and _bnb_available:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "    baseline_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "else:\n",
        "    dtype = torch.float32 if DEVICE == \"cpu\" else torch.float16\n",
        "    baseline_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=dtype)\n",
        "    baseline_model.to(DEVICE)\n",
        "\n",
        "baseline_model.eval()\n",
        "print(\"Baseline model loaded.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation helpers for Gemma chat\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a Python programming assistant.\"\n",
        "\n",
        "\n",
        "def build_prompt(tokenizer, question: str, system_prompt: Optional[str] = SYSTEM_PROMPT) -> str:\n",
        "    if system_prompt:\n",
        "        user_content = system_prompt.strip() + \"\\n\\n\" + question.strip()\n",
        "    else:\n",
        "        user_content = question.strip()\n",
        "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
        "    return baseline_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def ask_model(model, tokenizer, question: str, max_new_tokens: int = 400) -> str:\n",
        "    prompt = build_prompt(tokenizer, question)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        use_cache=True,\n",
        "    )\n",
        "    gen_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual query: Baseline model\n",
        "question = input(\"Enter your question for the BASELINE model: \").strip()\n",
        "if not question:\n",
        "    raise ValueError(\"Please enter a non-empty question.\")\n",
        "print(\"=== BASELINE RESPONSE ===\")\n",
        "print(ask_model(baseline_model, baseline_tokenizer, question))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Locate dataset (adjust if running locally vs Colab)\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_PATHS = [\n",
        "    Path(\"data/processed/fine_tuning_train-v5.jsonl\"),\n",
        "    Path(\"/content/data/processed/fine_tuning_train-v5.jsonl\"),\n",
        "    Path(\"/content/fine_tuning_train-v5.jsonl\"),\n",
        "]\n",
        "\n",
        "DATA_PATH = None\n",
        "for p in DATA_PATHS:\n",
        "    if p.exists():\n",
        "        DATA_PATH = str(p)\n",
        "        break\n",
        "\n",
        "if DATA_PATH is None:\n",
        "    raise FileNotFoundError(\"Dataset not found. Upload fine_tuning_train-v5.jsonl or mount drive and set DATA_PATH.\")\n",
        "else:\n",
        "    print(\"Using dataset:\", DATA_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load JSONL chat-style dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_ds = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
        "print(raw_ds)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map messages -> single chat template string for supervised LM\n",
        "from datasets import Dataset\n",
        "\n",
        "def format_example(example):\n",
        "    msgs = example.get(\"messages\")\n",
        "    if not msgs:\n",
        "        return \"\"\n",
        "    try:\n",
        "        text = baseline_tokenizer.apply_chat_template(\n",
        "            msgs,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "    except Exception:\n",
        "        # Fallback: naive concatenation\n",
        "        parts = []\n",
        "        for m in msgs:\n",
        "            role = m.get(\"role\", \"user\")\n",
        "            parts.append(f\"{role}: {m.get('content','')}\")\n",
        "        text = \"\\n\".join(parts)\n",
        "    eos = baseline_tokenizer.eos_token or \"</s>\"\n",
        "    return text + eos\n",
        "\n",
        "\n",
        "def formatting_func(batch):\n",
        "    texts = []\n",
        "    for msgs in batch[\"messages\"]:\n",
        "        texts.append(format_example({\"messages\": msgs}))\n",
        "    return {\"text\": texts}\n",
        "\n",
        "processed = raw_ds.map(formatting_func, batched=True, remove_columns=raw_ds.column_names)\n",
        "print(processed)\n",
        "print(\"Sample:\\n\", processed[0][\"text\"].split(\"\\n\")[:6])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QLoRA setup and Trainer\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "MODEL_NAME = MODEL_ID\n",
        "OUTPUT_DIR = \"outputs/gemma2-2b-it-lora-v5\"\n",
        "\n",
        "# 1) 4-bit quantization config (T4-safe compute dtype)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# 2) Reload a clean base model for training in 4-bit\n",
        "train_base = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "train_base.config.use_cache = False  # disable cache during training\n",
        "train_base = prepare_model_for_kbit_training(train_base)\n",
        "\n",
        "# 3) LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# 4) Wrap with LoRA\n",
        "train_model = get_peft_model(train_base, lora_config)\n",
        "train_model.print_trainable_parameters()\n",
        "\n",
        "# 5) Tokenizer (pad to eos)\n",
        "train_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "train_tokenizer.pad_token = train_tokenizer.eos_token\n",
        "\n",
        "# 6) Tokenize dataset\n",
        "MAX_LEN = 1024\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return train_tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "tokenized_dataset = processed.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=processed.column_names,\n",
        ")\n",
        "\n",
        "# 7) Data collator\n",
        "collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=train_tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# 8) TrainingArguments — train longer, safe dtypes for T4\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    bf16=False,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# 9) Train\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=train_model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# 10) Save adapter + tokenizer\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "train_tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved adapter to:\", OUTPUT_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load fine-tuned adapter for inference (fresh base -> attach adapter)\n",
        "from peft import PeftModel\n",
        "\n",
        "# Fresh base (same quantization as training)\n",
        "inf_base = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(inf_base, OUTPUT_DIR)\n",
        "ft_model.eval()\n",
        "\n",
        "ft_tokenizer = train_tokenizer  # reuse tokenizer saved during training\n",
        "print(\"Fine-tuned adapter loaded.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual query: Fine-tuned model\n",
        "question = input(\"Enter your question for the FINE-TUNED model: \").strip()\n",
        "if not question:\n",
        "    raise ValueError(\"Please enter a non-empty question.\")\n",
        "print(\"=== FINE-TUNED RESPONSE ===\")\n",
        "print(ask_model(ft_model, ft_tokenizer, question))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
