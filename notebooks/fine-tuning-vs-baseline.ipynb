{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning vs Baseline (Gemma 2 2B Instruct)\n",
        "\n",
        "This notebook compares a baseline instruction-tuned LLM against a fine-tuned version on time-sensitive Python release questions.\n",
        "\n",
        "Sections:\n",
        "- Baseline: load the same model used in your RAG notebook and ask the 6 demo questions\n",
        "- Fine-tuning: train a LoRA/QLoRA adapter using your JSONL dataset and re-evaluate the same questions\n",
        "\n",
        "Notes:\n",
        "- Target model: `google/gemma-2-2b-it` (same as your RAG notebook)\n",
        "- Dataset: `data/processed/fine-tuning-training-data.v4.cleaned.jsonl` (or upload via Colab)\n",
        "- Designed for Google Colab (T4/L4/A100); runs with 4-bit quantization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# If running on Colab, install/upgrade required packages\n",
        "pip -q install -U transformers accelerate peft trl bitsandbytes datasets sentencepiece protobuf pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Hugging Face login (only if your model is gated)\n",
        "# from huggingface_hub import login\n",
        "# login(token=\"<hf_token>\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, json, math\n",
        "from typing import List, Dict, Optional\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "\n",
        "try:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    _bnb_available = True\n",
        "except Exception:\n",
        "    _bnb_available = False\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_ID = \"google/gemma-2-2b-it\"  # same baseline as RAG notebook\n",
        "\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"Model:\", MODEL_ID)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "\n",
        "if DEVICE == \"cuda\" and _bnb_available:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
        "    )\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "else:\n",
        "    dtype = torch.float32 if DEVICE == \"cpu\" else torch.float16\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=dtype)\n",
        "    base_model.to(DEVICE)\n",
        "\n",
        "base_model.eval()\n",
        "print(\"Baseline model loaded.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GEN_CFG = {\n",
        "    \"max_new_tokens\": 600,\n",
        "    \"temperature\": 0.3,\n",
        "    \"top_p\": 0.9,\n",
        "    \"repetition_penalty\": 1.1,\n",
        "}\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a Python programming assistant.\"\n",
        "\n",
        "\n",
        "def _format_chat(messages: List[Dict[str, str]], add_generation_prompt: bool = True) -> Dict[str, torch.Tensor]:\n",
        "    if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
        "        effective_messages = messages\n",
        "        if messages and messages[0].get(\"role\") == \"system\":\n",
        "            system_text = messages[0][\"content\"]\n",
        "            effective_messages = messages[1:]\n",
        "            if effective_messages and effective_messages[0].get(\"role\") == \"user\":\n",
        "                effective_messages = effective_messages.copy()\n",
        "                effective_messages[0] = {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"{system_text}\\n\\n{effective_messages[0]['content']}\"\n",
        "                }\n",
        "            else:\n",
        "                effective_messages = [{\"role\": \"user\", \"content\": system_text}]\n",
        "        prompt_text = tokenizer.apply_chat_template(\n",
        "            effective_messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=add_generation_prompt\n",
        "        )\n",
        "    else:\n",
        "        sys_msg = \"\"\n",
        "        if messages and messages[0].get(\"role\") == \"system\":\n",
        "            sys_msg = f\"System: {messages[0]['content']}\\n\"\n",
        "            user_msgs = messages[1:]\n",
        "        else:\n",
        "            user_msgs = messages\n",
        "        convo = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in user_msgs])\n",
        "        prompt_text = (sys_msg + convo + (\"\\nAssistant:\" if add_generation_prompt else \"\"))\n",
        "\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_from_messages(\n",
        "    model,\n",
        "    messages: List[Dict[str, str]],\n",
        "    max_new_tokens: int = GEN_CFG[\"max_new_tokens\"],\n",
        "    temperature: float = GEN_CFG[\"temperature\"],\n",
        "    top_p: float = GEN_CFG[\"top_p\"],\n",
        "    repetition_penalty: float = GEN_CFG[\"repetition_penalty\"],\n",
        ") -> str:\n",
        "    inputs = _format_chat(messages, add_generation_prompt=True)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    gen_ids = outputs[0][input_len:]\n",
        "    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def ask_baseline(question: str, system_prompt: Optional[str] = SYSTEM_PROMPT, **gen_kwargs) -> str:\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    return generate_from_messages(base_model, messages, **gen_kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEMO_QUESTIONS = [\n",
        "    \"What was added in Python 3.12.2 released in March 2024?\",\n",
        "    \"What specific bug fixes and security advisories were included in Python 3.12.3 (April 2024)?\",\n",
        "    \"Which issues and regressions were addressed in Python 3.13.1 and 3.13.2, and on what dates were they released?\",\n",
        "    \"Which CVEs were fixed in Python 3.12.x during midâ€‘2024, and which modules were impacted?\",\n",
        "    \"Which PEPs targeting Python 3.14 changed status between alpha and beta, and what wording changed in their Accepted texts?\",\n",
        "    \"What were the documented release blockers and notable open issues listed before the Python 3.13.1 release, and which were resolved by that release?\",\n",
        "]\n",
        "\n",
        "for i, q in enumerate(DEMO_QUESTIONS, 1):\n",
        "    print(f\"\\nQ{i}: {q}\\n\")\n",
        "    ans = ask_baseline(q)\n",
        "    print(ans)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Fine-tuning with your JSONL dataset (QLoRA)\n",
        "We will fine-tune the same baseline model using your dataset:\n",
        "- Preferred path: `data/processed/fine-tuning-training-data.v4.cleaned.jsonl`\n",
        "- If not found, you can upload the file when running in Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Locate dataset (adjust this if running locally vs Colab)\n",
        "from pathlib import Path\n",
        "\n",
        "# Preferred local path\n",
        "DATA_PATHS = [\n",
        "    Path(\"data/processed/fine-tuning-training-data.v4.cleaned.jsonl\"),\n",
        "    Path(\"/content/data/processed/fine-tuning-training-data.v4.cleaned.jsonl\"),\n",
        "    Path(\"/content/fine-tuning-training-data.v4.cleaned.jsonl\"),\n",
        "]\n",
        "\n",
        "DATA_PATH = None\n",
        "for p in DATA_PATHS:\n",
        "    if p.exists():\n",
        "        DATA_PATH = str(p)\n",
        "        break\n",
        "\n",
        "if DATA_PATH is None:\n",
        "    print(\"Dataset not found at default paths. Upload the JSONL file or mount drive and set DATA_PATH manually.\")\n",
        "else:\n",
        "    print(\"Using dataset:\", DATA_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load JSONL chat-style dataset with datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = None\n",
        "if DATA_PATH is not None:\n",
        "    dataset = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
        "    print(dataset)\n",
        "else:\n",
        "    raise FileNotFoundError(\"Please set DATA_PATH to your JSONL file.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Formatting function: convert messages -> chat template string\n",
        "# We keep it simple for demo: train on full conversation (prompt + answer)\n",
        "# For production, you can mask inputs using TRL's response_template.\n",
        "\n",
        "def format_example(example):\n",
        "    msgs = example.get(\"messages\")\n",
        "    if not msgs:\n",
        "        return \"\"\n",
        "    try:\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            msgs,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "    except Exception:\n",
        "        # Fallback: naive concatenation\n",
        "        parts = []\n",
        "        for m in msgs:\n",
        "            role = m.get(\"role\", \"user\")\n",
        "            parts.append(f\"{role}: {m.get('content','')}\")\n",
        "        text = \"\\n\".join(parts)\n",
        "    # Ensure an EOS to bound samples\n",
        "    eos = tokenizer.eos_token or \"</s>\"\n",
        "    return text + eos\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "def formatting_func(examples):\n",
        "    texts = []\n",
        "    for msgs in examples[\"messages\"]:\n",
        "        texts.append(format_example({\"messages\": msgs}))\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Map to a text field for SFTTrainer\n",
        "processed = dataset.map(formatting_func, batched=True, remove_columns=dataset.column_names)\n",
        "print(processed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QLoRA configuration and SFT training (TRL + PEFT)\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "OUTPUT_DIR = \"outputs/gemma2-2b-it-lora\"\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "        \"gate_proj\",\"up_proj\",\"down_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Training arguments (tune for your GPU)\n",
        "train_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,               # increase to 2-3 if you have time\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    bf16=(torch.cuda.is_available()),\n",
        "    fp16=not torch.cuda.is_available(),\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "# Use the already-quantized base_model when on CUDA+bnb\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=processed,\n",
        "    args=train_args,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=False,\n",
        "    peft_config=lora_config,\n",
        "    max_seq_length=2048,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the LoRA adapter\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved adapter to:\", OUTPUT_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the fine-tuned adapter for inference\n",
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
        "ft_model.eval()\n",
        "print(\"Loaded LoRA adapter for inference.\")\n",
        "\n",
        "\n",
        "def ask_finetuned(question: str, system_prompt: Optional[str] = SYSTEM_PROMPT, **gen_kwargs) -> str:\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    return generate_from_messages(ft_model, messages, **gen_kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-ask the same demo questions: baseline vs fine-tuned\n",
        "for i, q in enumerate(DEMO_QUESTIONS, 1):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Q{i}: {q}\")\n",
        "    print(\"-\"*80)\n",
        "    base = ask_baseline(q)\n",
        "    print(\"Baseline:\\n\", base)\n",
        "    print(\"-\"*80)\n",
        "    ft = ask_finetuned(q)\n",
        "    print(\"Fine-tuned:\\n\", ft)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
