{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxjcvZve0wyE"
      },
      "source": [
        "# Baseline with RAG Integration + Chroma (persistent vectors)\n",
        "\n",
        "This notebook is an exact copy of `baseline_with_rag_integration.ipynb` plus additional cells to persist and preview vector embeddings using Chroma. The retrieval, chunking, and answering flow remains unchanged.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dpe00ek0wyG"
      },
      "source": [
        "## Original baseline cells (unchanged)\n",
        "The cells below are copied 1:1 from `baseline_with_rag_integration.ipynb`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yKos8Z40wyG"
      },
      "source": [
        "# Baseline with RAG Integration\n",
        "\n",
        "This notebook extends the baseline LLM with retrieval-augmented generation (RAG).\n",
        "\n",
        "Workflow:\n",
        "- Load the same baseline model used in the prompt-engineering notebook\n",
        "- Ingest a user-provided knowledge base (e.g., JSONL/CSV/TXT with recent release info)\n",
        "- Build embeddings + FAISS index\n",
        "- Compare answers: baseline `ask()` vs RAG `ask_rag()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "z_7G8s-X0wyG"
      },
      "outputs": [],
      "source": [
        "# Installs (Colab/Local)\n",
        "%pip -q install -U transformers accelerate sentencepiece bitsandbytes sentence-transformers faiss-cpu pandas python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX7tGkCg0wyH"
      },
      "outputs": [],
      "source": [
        "# Optional: Hugging Face login (needed if your chosen model is gated)\n",
        "from huggingface_hub import login\n",
        "login(token=\"access token\")  # uncomment and add your token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "d37e554fe2794976b08646696c634d5a",
            "101197d033374fc7a0b9b8173ea3a828",
            "41c4c078ad274cdbaacf16bfcc6f27b7",
            "edf5f6d8ecb14dcaad194458c778aa6c",
            "0e1bf2fd48f8403a8986ca4e2f50f390",
            "dac1491df4a04f578fb506e64d7d8483",
            "97995679eec244efac93f56250fb7a8a",
            "9147cc3856154afa8da4a82c3f9593d7",
            "a45cf17ccf454c32bc16f4ac34747e6a",
            "a6a6cd28125543cebfd841091f39a42b",
            "2d345e17d36640b78a00d405c4d2dfd1"
          ]
        },
        "id": "JG3spWld0wyI",
        "outputId": "55386841-36e1-4820-cb9e-15f9f576e65f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Model: google/gemma-2-2b-it\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d37e554fe2794976b08646696c634d5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "try:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    _bnb_available = True\n",
        "except Exception:\n",
        "    _bnb_available = False\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_ID = \"google/gemma-2-2b-it\"  # same baseline\n",
        "\n",
        "# Generation defaults\n",
        "GEN_CFG = {\n",
        "    \"max_new_tokens\": 800,\n",
        "    \"temperature\": 0.3,\n",
        "    \"top_p\": 0.9,\n",
        "    \"repetition_penalty\": 1.1,\n",
        "}\n",
        "\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"Model:\", MODEL_ID)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "\n",
        "if DEVICE == \"cuda\" and _bnb_available:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "else:\n",
        "    dtype = torch.float32 if DEVICE == \"cpu\" else torch.float16\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=dtype)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "model.eval()\n",
        "print(\"Model loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4emBpFsb0wyI"
      },
      "outputs": [],
      "source": [
        "PYTHON_ASSISTANT_SYSTEM_PROMPT = \"You are a Python programming assistant.\"\n",
        "\n",
        "def _format_chat(messages: List[Dict[str, str]], add_generation_prompt: bool = True) -> Dict[str, torch.Tensor]:\n",
        "    if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
        "        effective_messages = messages\n",
        "        if messages and messages[0].get(\"role\") == \"system\":\n",
        "            system_text = messages[0][\"content\"]\n",
        "            effective_messages = messages[1:]\n",
        "            if effective_messages and effective_messages[0].get(\"role\") == \"user\":\n",
        "                effective_messages = effective_messages.copy()\n",
        "                effective_messages[0] = {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"{system_text}\\n\\n{effective_messages[0]['content']}\"\n",
        "                }\n",
        "            else:\n",
        "                effective_messages = [{\"role\": \"user\", \"content\": system_text}]\n",
        "        prompt_text = tokenizer.apply_chat_template(\n",
        "            effective_messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=add_generation_prompt\n",
        "        )\n",
        "    else:\n",
        "        sys_msg = \"\"\n",
        "        if messages and messages[0].get(\"role\") == \"system\":\n",
        "            sys_msg = f\"System: {messages[0]['content']}\\n\"\n",
        "            user_msgs = messages[1:]\n",
        "        else:\n",
        "            user_msgs = messages\n",
        "        convo = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in user_msgs])\n",
        "        prompt_text = (sys_msg + convo + (\"\\nAssistant:\" if add_generation_prompt else \"\"))\n",
        "\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_from_messages(\n",
        "    messages: List[Dict[str, str]],\n",
        "    max_new_tokens: int = GEN_CFG[\"max_new_tokens\"],\n",
        "    temperature: float = GEN_CFG[\"temperature\"],\n",
        "    top_p: float = GEN_CFG[\"top_p\"],\n",
        "    repetition_penalty: float = GEN_CFG[\"repetition_penalty\"],\n",
        ") -> str:\n",
        "    inputs = _format_chat(messages, add_generation_prompt=True)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    gen_ids = outputs[0][input_len:]\n",
        "    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "    return text.strip()\n",
        "\n",
        "def ask(question: str, system_prompt: Optional[str] = PYTHON_ASSISTANT_SYSTEM_PROMPT, **gen_kwargs) -> str:\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    return generate_from_messages(messages, **gen_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "l6mwhWZM0wyI"
      },
      "outputs": [],
      "source": [
        "# Knowledge base ingestion\n",
        "# Accepts: JSONL, CSV('content' col), TXT/MD\n",
        "from pathlib import Path\n",
        "from typing import Union\n",
        "\n",
        "def read_jsonl(path: Union[str, Path]) -> List[Dict[str, str]]:\n",
        "    records = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            obj = json.loads(line)\n",
        "            content = obj.get(\"content\") or obj.get(\"text\") or obj.get(\"body\")\n",
        "            if not content:\n",
        "                continue\n",
        "            records.append({\n",
        "                \"id\": obj.get(\"id\") or obj.get(\"_id\") or str(len(records)),\n",
        "                \"title\": obj.get(\"title\") or \"\",\n",
        "                \"content\": content,\n",
        "                \"meta\": {k: v for k, v in obj.items() if k not in {\"id\", \"_id\", \"title\", \"content\", \"text\", \"body\"}}\n",
        "            })\n",
        "    return records\n",
        "\n",
        "def read_csv(path: Union[str, Path]) -> List[Dict[str, str]]:\n",
        "    df = pd.read_csv(path)\n",
        "    if \"content\" not in df.columns:\n",
        "        raise ValueError(\"CSV must have a 'content' column\")\n",
        "    recs = []\n",
        "    for i, row in df.iterrows():\n",
        "        recs.append({\n",
        "            \"id\": str(row.get(\"id\", i)),\n",
        "            \"title\": str(row.get(\"title\", \"\")),\n",
        "            \"content\": str(row[\"content\"]),\n",
        "            \"meta\": {k: row[k] for k in df.columns if k not in {\"id\", \"title\", \"content\"}}\n",
        "        })\n",
        "    return recs\n",
        "\n",
        "def read_texts(paths: List[Union[str, Path]]) -> List[Dict[str, str]]:\n",
        "    recs = []\n",
        "    for p in paths:\n",
        "        p = Path(p)\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "        recs.append({\n",
        "            \"id\": p.stem,\n",
        "            \"title\": p.name,\n",
        "            \"content\": text,\n",
        "            \"meta\": {\"path\": str(p)}\n",
        "        })\n",
        "    return recs\n",
        "\n",
        "def load_kb(path_or_dir: Union[str, Path]) -> List[Dict[str, str]]:\n",
        "    p = Path(path_or_dir)\n",
        "    if p.is_dir():\n",
        "        files = list(p.rglob(\"*.jsonl\")) + list(p.rglob(\"*.csv\")) + list(p.rglob(\"*.txt\")) + list(p.rglob(\"*.md\"))\n",
        "        recs: List[Dict[str, str]] = []\n",
        "        for fpath in files:\n",
        "            if fpath.suffix == \".jsonl\":\n",
        "                recs.extend(read_jsonl(fpath))\n",
        "            elif fpath.suffix == \".csv\":\n",
        "                recs.extend(read_csv(fpath))\n",
        "            else:\n",
        "                recs.extend(read_texts([fpath]))\n",
        "        return recs\n",
        "    else:\n",
        "        if p.suffix == \".jsonl\":\n",
        "            return read_jsonl(p)\n",
        "        if p.suffix == \".csv\":\n",
        "            return read_csv(p)\n",
        "        if p.suffix in {\".txt\", \".md\"}:\n",
        "            return read_texts([p])\n",
        "        raise ValueError(\"Unsupported KB format; use .jsonl, .csv, .txt, .md or a directory containing them\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mFVemkk0wyJ",
        "outputId": "e1824c15-6c9f-4b8b-94cd-b2cbf2034895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        }
      ],
      "source": [
        "# Embeddings + FAISS index (unchanged)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"  # small, fast\n",
        "\n",
        "class VectorStore:\n",
        "    def __init__(self, dim: int):\n",
        "        self.dim = dim\n",
        "        self.index = faiss.IndexFlatIP(dim)  # cosine via normalized dot product\n",
        "        self.docs: List[Dict[str, str]] = []\n",
        "\n",
        "    def add(self, embeddings: np.ndarray, docs: List[Dict[str, str]]):\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "        self.docs.extend(docs)\n",
        "\n",
        "    def search(self, query_emb: np.ndarray, k: int = 5) -> List[Tuple[int, float]]:\n",
        "        faiss.normalize_L2(query_emb)\n",
        "        D, I = self.index.search(query_emb, k)\n",
        "        return list(zip(I[0].tolist(), D[0].tolist()))\n",
        "\n",
        "embedder = SentenceTransformer(EMBED_MODEL_ID)\n",
        "emb_dim = embedder.get_sentence_embedding_dimension()\n",
        "store = VectorStore(dim=emb_dim)\n",
        "\n",
        "def build_index(records: List[Dict[str, str]], batch_size: int = 64):\n",
        "    texts = [r[\"content\"] for r in records]\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        chunk = texts[i:i+batch_size]\n",
        "        vecs = embedder.encode(chunk, convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=False)\n",
        "        embeddings.append(vecs)\n",
        "    if not embeddings:\n",
        "        return\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    store.add(embeddings, records)\n",
        "\n",
        "print(\"Embedding model:\", EMBED_MODEL_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8XYa_0rn0wyJ"
      },
      "outputs": [],
      "source": [
        "# Retrieval and RAG ask() (unchanged)\n",
        "from textwrap import dedent\n",
        "\n",
        "def embed(texts: List[str]) -> np.ndarray:\n",
        "    vecs = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=False)\n",
        "    return vecs\n",
        "\n",
        "def retrieve(query: str, k: int = 4) -> List[Dict[str, str]]:\n",
        "    qvec = embed([query])\n",
        "    hits = store.search(qvec, k=k)\n",
        "    results = []\n",
        "    for idx, score in hits:\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        doc = store.docs[idx]\n",
        "        results.append({\"score\": float(score), **doc})\n",
        "    return results\n",
        "\n",
        "def make_context(snippets: List[Dict[str, str]]) -> str:\n",
        "    blocks = []\n",
        "    for s in snippets:\n",
        "        title = s.get(\"title\") or s.get(\"id\") or \"\"\n",
        "        prefix = f\"Title: {title}\\n\" if title else \"\"\n",
        "        blocks.append(prefix + s[\"content\"])\n",
        "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
        "\n",
        "def answer_with_context(question: str, context: str) -> str:\n",
        "    sys = PYTHON_ASSISTANT_SYSTEM_PROMPT\n",
        "    prompt = dedent(f\"\"\"\n",
        "    Use the context below to answer the user question. If the answer is not in the context, say you don't know.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\")\n",
        "    return ask(prompt, system_prompt=sys, max_new_tokens=GEN_CFG[\"max_new_tokens\"], temperature=GEN_CFG[\"temperature\"])\n",
        "\n",
        "# Public API (unchanged)\n",
        "\n",
        "def ask_baseline(question: str) -> str:\n",
        "    return ask(question)\n",
        "\n",
        "def ask_rag(question: str, k: int = 4) -> Tuple[str, List[Dict[str, str]]]:\n",
        "    snippets = retrieve(question, k=k)\n",
        "    context = make_context(snippets)\n",
        "    answer = answer_with_context(question, context)\n",
        "    return answer, snippets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk0Wyu7f0wyJ",
        "outputId": "923ae1b3-b993-461d-b512-91125a6abdf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 7 records from KB.\n",
            "Indexed 7 documents.\n",
            "\n",
            "Q: What were the documented release blockers and notable open issues listed before the Python 3.13.1 release, and which were resolved by that release?\n",
            "\n",
            "Baseline:\n",
            " I do not have access to real-time information, including specific details about past releases of software like Python.  This means I can't provide you with a list of documented release blockers and open issues for Python 3.13.1. \n",
            "\n",
            "**However, I can give you general information on how such information is typically handled:**\n",
            "\n",
            "* **Release Notes:** The official Python website (https://www.python.org/) will have detailed release notes for each new version, including:\n",
            "    * **Blockers:** Issues that prevented the release from progressing. These might be major bugs or compatibility problems.\n",
            "    * **Open Issues:** Remaining unresolved issues that need attention. This helps track progress and future development.\n",
            "* **GitHub Issues:** The Python project's GitHub repository (https://github.com/python/cpython) often has an \"Issues\" section where users report bugs, propose features, and discuss potential blockers. You can find this by searching for the specific Python version.\n",
            "* **Mailing Lists:** The Python community uses mailing lists (like the Python Developers mailing list) to communicate about the language's development. These lists often contain discussions about release blockers and open issues.\n",
            "\n",
            "\n",
            "**How to Find Information:**\n",
            "\n",
            "1. **Visit the Official Website:** Go to https://www.python.org/. Look for the \"Releases\" or \"News\" sections.\n",
            "2. **Check the GitHub Repository:** Navigate to the Python project's GitHub page (https://github.com/python/cpython). Search for the relevant version in the \"Issues\" tab. \n",
            "3. **Search Mailing List Archives:** Use search tools available on the Python Developer mailing list archives. \n",
            "\n",
            "\n",
            "\n",
            "Let me know if you have any other questions!\n",
            "\n",
            "RAG:\n",
            " According to the provided context, here's the information about release blockers and notable open issues before the Python 3.13.1 release:\n",
            "\n",
            "* **Release Blockers:**\n",
            "    * **Managed Dict Crash:**  A crash during managed dict rematerialization\n",
            "    * **PyEval_SetTrace Segfault:** A segmentation fault when `PyEval_SetTrace` was called with a null Python frame\n",
            "    * **Free-Threaded Assertion:** A negative refcount assertion failure in the free-threaded build\n",
            "    * **Type Lookup Race:** A specific race condition in `_PyType_Lookup` \n",
            "\n",
            "* **Resolved by 3.13.1 Release:**\n",
            "   All of these issues were resolved by the 3.13.1 release date.\n",
            "\n",
            "Context snippets used (top-4):\n",
            "[1] id=q6_3131_release_blockers title=Python 3.13.1 Release Blockers and Notable Resolved Issues score=0.709\n",
            "[2] id=q3_py3132_release_details title=Python 3.13.2 Release Details (Date, Issues, Regressions) score=0.677\n",
            "[3] id=q1_py3122 title=Python 3.12.2 (March 2024) – Patch contents score=0.626\n",
            "[4] id=q2_py3123 title=Python 3.12.3 (April 2024) – Bug fixes and security advisories score=0.550\n"
          ]
        }
      ],
      "source": [
        "# Demo (unchanged): load KB, build index, compare baseline vs RAG\n",
        "KB_PATH = \"/content/updated_python_kb.jsonl\"  # change to your file/dir\n",
        "\n",
        "records = load_kb(KB_PATH)\n",
        "print(f\"Loaded {len(records)} records from KB.\")\n",
        "\n",
        "store = VectorStore(dim=emb_dim)\n",
        "build_index(records)\n",
        "print(f\"Indexed {len(store.docs)} documents.\")\n",
        "\n",
        "question = \"What were the documented release blockers and notable open issues listed before the Python 3.13.1 release, and which were resolved by that release?\"\n",
        "\n",
        "print(\"\\nQ:\", question)\n",
        "base_answer = ask_baseline(question)\n",
        "rag_answer, used = ask_rag(question, k=4)\n",
        "print(\"\\nBaseline:\\n\", base_answer)\n",
        "print(\"\\nRAG:\\n\", rag_answer)\n",
        "print(\"\\nContext snippets used (top-4):\")\n",
        "for i, snip in enumerate(used, 1):\n",
        "    print(f\"[{i}] id={snip.get('id')} title={snip.get('title','')[:80]} score={snip.get('score'):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BudtVMst0wyK"
      },
      "source": [
        "---\n",
        "\n",
        "## Chroma integration\n",
        "Persists the same embeddings and documents to `data/chroma/` for live preview during your demo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZecxUvLS0wyK",
        "outputId": "7345deea-e747-4f31-d104-538ad065d057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chroma persisted 7 docs to data/chroma/\n"
          ]
        }
      ],
      "source": [
        "%pip -q install chromadb\n",
        "\n",
        "import os, json, chromadb\n",
        "os.makedirs(\"data/chroma\", exist_ok=True)\n",
        "\n",
        "# Use the new PersistentClient API\n",
        "client = chromadb.PersistentClient(path=\"data/chroma\")\n",
        "col = client.get_or_create_collection(name=\"python_kb\")\n",
        "\n",
        "# Sanitize metadata to scalars (Chroma requires str/int/float/bool/None)\n",
        "def sanitize_meta(m):\n",
        "    out = {}\n",
        "    for k, v in (m or {}).items():\n",
        "        if isinstance(v, (str, int, float, bool)) or v is None:\n",
        "            out[k] = v\n",
        "        elif isinstance(v, (list, dict)):\n",
        "            out[k] = json.dumps(v, ensure_ascii=False)  # or \";\".join(v) for lists\n",
        "        else:\n",
        "            out[k] = str(v)\n",
        "    return out\n",
        "\n",
        "# Upsert the same records and vectors already built above\n",
        "_docs = [r[\"content\"] for r in records]\n",
        "_ids = [str(r[\"id\"]) for r in records]\n",
        "_metas = [{\"title\": r.get(\"title\", \"\"), **sanitize_meta(r.get(\"meta\"))} for r in records]\n",
        "_vecs = embedder.encode(_docs, convert_to_numpy=True).tolist()\n",
        "\n",
        "col.upsert(ids=_ids, documents=_docs, metadatas=_metas, embeddings=_vecs)\n",
        "print(f\"Chroma persisted {len(_ids)} docs to data/chroma/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWOny5bS0wyK",
        "outputId": "f9698197-2b7a-4774-a499-6cc983097b64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored doc emb dim: 384 preview: [-0.05569929 -0.0222838  -0.0356492   0.0378988   0.01280342 -0.1074873\n",
            " -0.0390874   0.01382053] title: Python 3.12.2 (March 2024) – Patch contents\n",
            "Query emb dim: 384 preview: [-0.061740849167108536, 0.023628272116184235, -0.09414426237344742, 0.00542877521365881, 0.05634896084666252, 0.03960898891091347, -0.03972223401069641, 0.10453187674283981]\n",
            "1 cos_sim≈ -0.08608782291412354 title= PEPs targeting Python 3.14: Status changes (alpha to beta)\n",
            "2 cos_sim≈ -0.29325008392333984 title= Python 3.13.1 Release Details (Date, Issues, Regressions)\n",
            "3 cos_sim≈ -0.3012566566467285 title= Python 3.13.2 Release Details (Date, Issues, Regressions)\n"
          ]
        }
      ],
      "source": [
        "# Preview: show a stored embedding and a query result\n",
        "res = col.get(ids=[_ids[0]], include=[\"embeddings\",\"metadatas\",\"documents\"])\n",
        "emb = res[\"embeddings\"][0]\n",
        "print(\"Stored doc emb dim:\", len(emb), \"preview:\", emb[:8], \"title:\", res[\"metadatas\"][0].get(\"title\"))\n",
        "\n",
        "q = \"What PEP replaced PEP 722 for inline script metadata?\"\n",
        "qvec = embedder.encode([q], convert_to_numpy=True).tolist()\n",
        "qout = col.query(query_embeddings=qvec, n_results=3, include=[\"distances\",\"documents\",\"metadatas\",\"embeddings\"])\n",
        "print(\"Query emb dim:\", len(qvec[0]), \"preview:\", qvec[0][:8])\n",
        "for i,(doc,meta,dist) in enumerate(zip(qout[\"documents\"][0], qout[\"metadatas\"][0], qout[\"distances\"][0]), 1):\n",
        "    print(i, \"cos_sim≈\", 1-dist, \"title=\", meta.get(\"title\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JM0sq6B0wyL"
      },
      "source": [
        "---\n",
        "\n",
        "## Optional: Gradio retrieval UI\n",
        "Quick interactive search to show nearest neighbors and cosine similarity during your demo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "Zyh1871e0wyL",
        "outputId": "dee4aa31-f054-4be3-8906-f37a9fd402ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, width, height, cache, element) => {\n                        if (!google.colab.kernel.accessAllowed && !cache) {\n                            return;\n                        }\n                        element.appendChild(document.createTextNode(''));\n                        const url = await google.colab.kernel.proxyPort(port, {cache});\n\n                        const external_link = document.createElement('div');\n                        external_link.innerHTML = `\n                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n                                    https://localhost:${port}${path}\n                                </a>\n                            </div>\n                        `;\n                        element.appendChild(external_link);\n\n                        const iframe = document.createElement('iframe');\n                        iframe.src = new URL(path, url).toString();\n                        iframe.height = height;\n                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n                        iframe.width = width;\n                        iframe.style.border = 0;\n                        element.appendChild(iframe);\n                    })(7861, \"/\", \"100%\", 500, false, window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip -q install gradio pandas\n",
        "import gradio as gr, pandas as pd\n",
        "\n",
        "def search_ui(q: str, k: int = 5):\n",
        "    if not q.strip():\n",
        "        return pd.DataFrame(columns=[\"title\",\"cosine_similarity\",\"snippet\"])\n",
        "    qvec = embedder.encode([q], convert_to_numpy=True).tolist()\n",
        "    out = col.query(query_embeddings=qvec, n_results=int(k), include=[\"distances\",\"metadatas\",\"documents\"])\n",
        "    rows = []\n",
        "    for meta, dist, doc in zip(out[\"metadatas\"][0], out[\"distances\"][0], out[\"documents\"][0]):\n",
        "        title = (meta or {}).get(\"title\", \"\")\n",
        "        snippet = (doc or \"\").replace(\"\\n\",\" \")[:160] + (\"...\" if len(doc or \"\") > 160 else \"\")\n",
        "        rows.append({\"title\": title, \"cosine_similarity\": round(1-float(dist), 4), \"snippet\": snippet})\n",
        "    return pd.DataFrame(rows, columns=[\"title\",\"cosine_similarity\",\"snippet\"])\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=search_ui,\n",
        "    inputs=[gr.Textbox(label=\"Query\"), gr.Slider(1, 10, value=5, step=1, label=\"Top K\")],\n",
        "    outputs=gr.Dataframe(label=\"Nearest neighbors\"),\n",
        "    title=\"Chroma Retrieval Demo\",\n",
        "    description=\"Type a question; see top-K neighbors with cosine similarity and snippets.\"\n",
        ")\n",
        "\n",
        "demo.launch(share=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e1bf2fd48f8403a8986ca4e2f50f390": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "101197d033374fc7a0b9b8173ea3a828": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dac1491df4a04f578fb506e64d7d8483",
            "placeholder": "​",
            "style": "IPY_MODEL_97995679eec244efac93f56250fb7a8a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2d345e17d36640b78a00d405c4d2dfd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41c4c078ad274cdbaacf16bfcc6f27b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9147cc3856154afa8da4a82c3f9593d7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a45cf17ccf454c32bc16f4ac34747e6a",
            "value": 2
          }
        },
        "9147cc3856154afa8da4a82c3f9593d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97995679eec244efac93f56250fb7a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a45cf17ccf454c32bc16f4ac34747e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6a6cd28125543cebfd841091f39a42b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37e554fe2794976b08646696c634d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_101197d033374fc7a0b9b8173ea3a828",
              "IPY_MODEL_41c4c078ad274cdbaacf16bfcc6f27b7",
              "IPY_MODEL_edf5f6d8ecb14dcaad194458c778aa6c"
            ],
            "layout": "IPY_MODEL_0e1bf2fd48f8403a8986ca4e2f50f390"
          }
        },
        "dac1491df4a04f578fb506e64d7d8483": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edf5f6d8ecb14dcaad194458c778aa6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6a6cd28125543cebfd841091f39a42b",
            "placeholder": "​",
            "style": "IPY_MODEL_2d345e17d36640b78a00d405c4d2dfd1",
            "value": " 2/2 [00:21&lt;00:00,  9.30s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
