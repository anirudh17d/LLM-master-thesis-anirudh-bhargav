{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced: Prompt Engineering + RAG v2 (Gemma-2B)\n",
        "\n",
        "This notebook preserves your original `rag_vs_promt_engineering-LATEST.ipynb` content unchanged below and adds improved helper functions and an embeddings-based retriever at the top.\n",
        "\n",
        "- Deterministic decoding with proper token slicing\n",
        "- Unified refusal policy and prompt template\n",
        "- Embeddings retriever (all-MiniLM-L6-v2) + FAISS with chunking\n",
        "- RAG v2 wrapper that conditions strictly on retrieved context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "!pip install -q transformers accelerate datasets sentence-transformers faiss-cpu scikit-learn huggingface_hub\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login, whoami\n",
        "import os, torch\n",
        "\n",
        "# Hugging Face auth: set HF_TOKEN env var OR paste directly below\n",
        "HF_TOKEN = (os.getenv(\"HF_TOKEN\", \"\").strip()) or \"\"  # paste your hf_... token between quotes if not using env var\n",
        "if HF_TOKEN:\n",
        "    try:\n",
        "        login(token=HF_TOKEN)\n",
        "        _u = whoami()\n",
        "        print(\"HF login OK:\", _u.get(\"name\") or _u.get(\"email\") or \"authenticated\")\n",
        "    except Exception as _e:\n",
        "        print(\"HF login failed:\", _e)\n",
        "else:\n",
        "    print(\"No HF token provided. Set HF_TOKEN env var or paste it in HF_TOKEN.\")\n",
        "\n",
        "def load_model_tokenizer(model_id: str = \"google/gemma-2b-it\", token: str | None = HF_TOKEN):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        token=token,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "MODEL_ID = \"google/gemma-2b-it\"\n",
        "model, tokenizer = load_model_tokenizer(MODEL_ID, token=HF_TOKEN)\n",
        "\n",
        "GEN_KW = dict(\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "def build_prompt_v2(user_question: str, context: str = \"\") -> str:\n",
        "    rules = (\n",
        "        \"You are a careful assistant. Follow STRICTLY:\\n\"\n",
        "        \"1) Only answer using the provided context (if any).\\n\"\n",
        "        \"2) If the answer is not in the context or you are uncertain, reply EXACTLY with: Sorry I do not have that information\\n\"\n",
        "        \"3) Do not add any explanation, punctuation, or extra words when refusing.\\n\"\n",
        "        \"4) When you do know the answer, explain it in detail with at least 3 sentences and examples if possible.\\n\"\n",
        "        \"5) Do not rephrase the refusal.\\n\"\n",
        "    )\n",
        "    if context.strip():\n",
        "        content = rules + f\"\\nUse ONLY this context to answer:\\n---\\n{context}\\n---\\n\\nQuestion: {user_question}\"\n",
        "    else:\n",
        "        content = rules + f\"\\nQuestion: {user_question}\"\n",
        "    messages = [{\"role\": \"user\", \"content\": content}]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_answer_v2(question_text: str, context: str = \"\") -> str:\n",
        "    prompt = build_prompt_v2(question_text, context)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    input_len = inputs.input_ids.shape[-1]\n",
        "    outputs = model.generate(**inputs, **GEN_KW)\n",
        "    gen_ids = outputs[0][input_len:]\n",
        "    answer = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "    if answer.lower().startswith(\"sorry i do not have that information\"):\n",
        "        return \"Sorry I do not have that information\"\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding retriever with FAISS\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss, numpy as np, math, json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "emb_model = SentenceTransformer(EMB_MODEL_NAME)\n",
        "\n",
        "class DocumentChunk:\n",
        "    def __init__(self, doc_id: str, chunk_id: int, text: str, meta: Dict[str, Any]):\n",
        "        self.doc_id = doc_id\n",
        "        self.chunk_id = chunk_id\n",
        "        self.text = text\n",
        "        self.meta = meta\n",
        "\n",
        "\n",
        "def chunk_text(text: str, max_tokens: int = 180, overlap: int = 30) -> List[str]:\n",
        "    # Simple whitespace chunking with overlap; token-approx via words\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return []\n",
        "    chunks = []\n",
        "    step = max_tokens - overlap\n",
        "    for i in range(0, len(words), step):\n",
        "        chunk_words = words[i : i + max_tokens]\n",
        "        if not chunk_words:\n",
        "            break\n",
        "        chunks.append(\" \".join(chunk_words))\n",
        "        if i + max_tokens >= len(words):\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def build_corpus(kb_records: List[Dict[str, Any]]) -> List[DocumentChunk]:\n",
        "    corpus: List[DocumentChunk] = []\n",
        "    for rec in kb_records:\n",
        "        content = (rec.get(\"title\", \"\") + \"\\n\" + rec.get(\"content\", \"\")).strip()\n",
        "        chunks = chunk_text(content, max_tokens=180, overlap=30)\n",
        "        for idx, ch in enumerate(chunks):\n",
        "            meta = {\n",
        "                \"title\": rec.get(\"title\", \"\"),\n",
        "                \"id\": rec.get(\"id\", \"\"),\n",
        "                \"version\": rec.get(\"version\", \"\"),\n",
        "                \"urls\": [s.get(\"url\") for s in rec.get(\"answer_card\", {}).get(\"sources\", [])],\n",
        "            }\n",
        "            corpus.append(DocumentChunk(str(rec.get(\"id\", \"\")), idx, ch, meta))\n",
        "    return corpus\n",
        "\n",
        "\n",
        "def build_faiss_index(chunks: List[DocumentChunk]):\n",
        "    texts = [c.text for c in chunks]\n",
        "    embeddings = emb_model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    index.add(embeddings)\n",
        "    return index, embeddings\n",
        "\n",
        "\n",
        "def retrieve_top_k(query: str, chunks: List[DocumentChunk], index, top_k: int = 3):\n",
        "    q_emb = emb_model.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    scores, idxs = index.search(q_emb, top_k)\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], idxs[0]):\n",
        "        ch = chunks[int(idx)]\n",
        "        results.append((float(score), ch))\n",
        "    return results\n",
        "\n",
        "\n",
        "def format_context_from_results(results) -> str:\n",
        "    blocks = []\n",
        "    for score, ch in results:\n",
        "        srcs = ch.meta.get(\"urls\") or []\n",
        "        title = ch.meta.get(\"title\", \"\")\n",
        "        header = f\"### {title} (score={score:.2f})\\n\"\n",
        "        src_line = (\"Sources: \" + \", \".join(srcs)) if srcs else \"\"\n",
        "        blocks.append(header + ch.text + (\"\\n\" + src_line if src_line else \"\"))\n",
        "    return \"\\n\\n\".join(blocks)\n",
        "\n",
        "# Optional: quick load if kb path available; else keep helpers ready\n",
        "KB_PATH = \"/content/python_release_kb.jsonl\"\n",
        "try:\n",
        "    kb_records = []\n",
        "    with open(KB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            kb_records.append(json.loads(line))\n",
        "    corpus_chunks = build_corpus(kb_records)\n",
        "    faiss_index, _ = build_faiss_index(corpus_chunks)\n",
        "except Exception as e:\n",
        "    kb_records, corpus_chunks, faiss_index = None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG v2 answer functions\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "def build_context_v2(query: str, top_k: int = 3) -> str:\n",
        "    assert corpus_chunks is not None and faiss_index is not None, \"Load KB and build index first.\"\n",
        "    results = retrieve_top_k(query, corpus_chunks, faiss_index, top_k=top_k)\n",
        "    return format_context_from_results(results)\n",
        "\n",
        "\n",
        "def rag_answer_v2(query: str, top_k: int = 3) -> str:\n",
        "    context = build_context_v2(query, top_k=top_k)\n",
        "    answer = generate_answer_v2(query, context=context)\n",
        "    display(Markdown(answer))\n",
        "    return answer\n",
        "\n",
        "# Smoke test cells (commented):\n",
        "# q = \"What PEP replaced PEP 722 for inline script metadata?\"\n",
        "# rag_answer_v2(q, top_k=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1: Prompt Engineering (no RAG)\n",
        "\n",
        "Run the setup cell above, then use the demo cells at the end of this section to ask questions without any retrieved context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt-only demo cells\n",
        "# 1) Generic question (should answer)\n",
        "print(generate_answer_v2(\"Explain list comprehensions in Python with a short example.\"))\n",
        "\n",
        "# 2) Domain-specific fact (should refuse without context)\n",
        "print(generate_answer_v2(\"What PEP replaced PEP 722 for inline script metadata?\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "Set your KB path, build the retriever index, and then ask the same question again with context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG setup: point to your KB and build index\n",
        "import json\n",
        "\n",
        "KB_PATH = \"data/processed/python_release_kb.jsonl\"  # adjust if needed\n",
        "kb_records = []\n",
        "with open(KB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        kb_records.append(json.loads(line))\n",
        "\n",
        "corpus_chunks = build_corpus(kb_records)\n",
        "faiss_index, _ = build_faiss_index(corpus_chunks)\n",
        "len(corpus_chunks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG demo: ask the same question\n",
        "rag_answer_v2(\"What PEP replaced PEP 722 for inline script metadata?\", top_k=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
